{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\npath = '/kaggle/input/col774-2022'\ntrain_x_path = path + '/train_x.csv'\ntrain_y_path = path + '/train_y.csv'\ntest_x_path = path + '/non_comp_test_x.csv'\ntest_y_path = path + '/non_comp_test_y.csv'","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-11-28T10:39:37.749468Z","iopub.execute_input":"2022-11-28T10:39:37.749936Z","iopub.status.idle":"2022-11-28T10:39:37.759909Z","shell.execute_reply.started":"2022-11-28T10:39:37.749889Z","shell.execute_reply":"2022-11-28T10:39:37.758657Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"/kaggle/input/col774-2022/train_x.csv\n","output_type":"stream"}]},{"cell_type":"code","source":"train_x_data = ((pd.read_csv(train_x_path))['Title'])\ntrain_y_data = ((pd.read_csv(train_y_path))['Genre'])\ntest_x_data = ((pd.read_csv(test_x_path))['Title'])\ntest_y_data = ((pd.read_csv(test_y_path))['Genre'])\n\ntotal_x_data = pd.concat([train_x_data,test_x_data], axis = 0)\ntotal_y_data = pd.concat([train_y_data, test_y_data], axis = 0)\nmodel_name = \"roberta-large\"","metadata":{"execution":{"iopub.status.busy":"2022-11-28T10:39:37.762647Z","iopub.execute_input":"2022-11-28T10:39:37.763470Z","iopub.status.idle":"2022-11-28T10:39:37.959748Z","shell.execute_reply.started":"2022-11-28T10:39:37.763436Z","shell.execute_reply":"2022-11-28T10:39:37.958759Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"data_dict_list = {}\ndata_dict_list[\"label\"] = total_y_data\ndata_dict_list[\"text\"] = total_x_data","metadata":{"execution":{"iopub.status.busy":"2022-11-28T10:39:37.972059Z","iopub.execute_input":"2022-11-28T10:39:37.972579Z","iopub.status.idle":"2022-11-28T10:39:37.980058Z","shell.execute_reply.started":"2022-11-28T10:39:37.972537Z","shell.execute_reply":"2022-11-28T10:39:37.979126Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"from datasets import Dataset\ndata_train = Dataset.from_dict(data_dict_list)","metadata":{"execution":{"iopub.status.busy":"2022-11-28T10:39:37.983649Z","iopub.execute_input":"2022-11-28T10:39:37.984099Z","iopub.status.idle":"2022-11-28T10:39:39.133098Z","shell.execute_reply.started":"2022-11-28T10:39:37.984067Z","shell.execute_reply":"2022-11-28T10:39:39.132098Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"from transformers import AutoTokenizer\n\ntokenizer = AutoTokenizer.from_pretrained(model_name)","metadata":{"execution":{"iopub.status.busy":"2022-11-28T10:39:39.134452Z","iopub.execute_input":"2022-11-28T10:39:39.135042Z","iopub.status.idle":"2022-11-28T10:39:43.045035Z","shell.execute_reply.started":"2022-11-28T10:39:39.134987Z","shell.execute_reply":"2022-11-28T10:39:43.044050Z"},"trusted":true},"execution_count":7,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/482 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"df50d6c014bb465aa6c21b0d93f85f68"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/878k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"367f38de90b24233a0aea9ab559df55e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/446k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a4adba9ce8ab445ab07dd139e88899d2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/1.29M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c8acbcd949f64f5b829407c9a44619c1"}},"metadata":{}}]},{"cell_type":"code","source":"def preprocess_function(examples):\n    return tokenizer(examples[\"text\"], truncation=True)","metadata":{"execution":{"iopub.status.busy":"2022-11-28T10:39:43.046513Z","iopub.execute_input":"2022-11-28T10:39:43.046880Z","iopub.status.idle":"2022-11-28T10:39:43.052073Z","shell.execute_reply.started":"2022-11-28T10:39:43.046843Z","shell.execute_reply":"2022-11-28T10:39:43.051060Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"tokenized_train = data_train.map(preprocess_function, batched=True)","metadata":{"execution":{"iopub.status.busy":"2022-11-28T10:39:43.053726Z","iopub.execute_input":"2022-11-28T10:39:43.054450Z","iopub.status.idle":"2022-11-28T10:39:45.178243Z","shell.execute_reply.started":"2022-11-28T10:39:43.054413Z","shell.execute_reply":"2022-11-28T10:39:45.177273Z"},"trusted":true},"execution_count":9,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/40 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8ef454bd80ab449db5a4702b500b4271"}},"metadata":{}}]},{"cell_type":"code","source":"from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\nmodel = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=30)","metadata":{"execution":{"iopub.status.busy":"2022-11-28T10:40:26.879312Z","iopub.execute_input":"2022-11-28T10:40:26.879687Z","iopub.status.idle":"2022-11-28T10:41:22.696324Z","shell.execute_reply.started":"2022-11-28T10:40:26.879653Z","shell.execute_reply":"2022-11-28T10:41:22.695368Z"},"trusted":true},"execution_count":11,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/1.33G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b92eff969eb8494683ae3de5f4df9310"}},"metadata":{}},{"name":"stderr","text":"Some weights of the model checkpoint at roberta-large were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'roberta.pooler.dense.weight', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'roberta.pooler.dense.bias', 'lm_head.dense.bias']\n- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.dense.weight', 'classifier.out_proj.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}]},{"cell_type":"code","source":"training_args = TrainingArguments(\n    output_dir=\"./results\",\n    learning_rate=2e-5,\n    per_device_train_batch_size=16,\n    per_device_eval_batch_size=16,\n    num_train_epochs=6,\n    weight_decay=0.01,\n    save_strategy = \"no\"\n)\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=tokenized_train,\n    eval_dataset=tokenized_train,\n    tokenizer=tokenizer,\n)","metadata":{"execution":{"iopub.status.busy":"2022-11-28T10:41:35.483291Z","iopub.execute_input":"2022-11-28T10:41:35.483652Z","iopub.status.idle":"2022-11-28T10:41:44.048413Z","shell.execute_reply.started":"2022-11-28T10:41:35.483621Z","shell.execute_reply":"2022-11-28T10:41:44.047188Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"os.environ[\"WANDB_DISABLED\"] = \"true\"\ntrainer.train()","metadata":{"execution":{"iopub.status.busy":"2022-11-28T10:41:48.911030Z","iopub.execute_input":"2022-11-28T10:41:48.911414Z","iopub.status.idle":"2022-11-28T12:30:39.270544Z","shell.execute_reply.started":"2022-11-28T10:41:48.911382Z","shell.execute_reply":"2022-11-28T12:30:39.269564Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stderr","text":"The following columns in the training set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n/opt/conda/lib/python3.7/site-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  FutureWarning,\n***** Running training *****\n  Num examples = 39900\n  Num Epochs = 7\n  Instantaneous batch size per device = 16\n  Total train batch size (w. parallel, distributed & accumulation) = 32\n  Gradient Accumulation steps = 1\n  Total optimization steps = 8729\nAutomatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n","output_type":"stream"},{"name":"stdout","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.12.21"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"W&B syncing is set to <code>`offline`<code> in this directory.  <br/>Run <code>`wandb online`<code> or set <code>WANDB_MODE=online<code> to enable cloud syncing."},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='8729' max='8729' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [8729/8729 1:48:34, Epoch 7/7]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>500</td>\n      <td>2.017500</td>\n    </tr>\n    <tr>\n      <td>1000</td>\n      <td>1.517300</td>\n    </tr>\n    <tr>\n      <td>1500</td>\n      <td>1.339600</td>\n    </tr>\n    <tr>\n      <td>2000</td>\n      <td>1.225200</td>\n    </tr>\n    <tr>\n      <td>2500</td>\n      <td>1.172100</td>\n    </tr>\n    <tr>\n      <td>3000</td>\n      <td>0.930500</td>\n    </tr>\n    <tr>\n      <td>3500</td>\n      <td>0.940500</td>\n    </tr>\n    <tr>\n      <td>4000</td>\n      <td>0.818500</td>\n    </tr>\n    <tr>\n      <td>4500</td>\n      <td>0.725000</td>\n    </tr>\n    <tr>\n      <td>5000</td>\n      <td>0.711200</td>\n    </tr>\n    <tr>\n      <td>5500</td>\n      <td>0.526800</td>\n    </tr>\n    <tr>\n      <td>6000</td>\n      <td>0.539000</td>\n    </tr>\n    <tr>\n      <td>6500</td>\n      <td>0.464600</td>\n    </tr>\n    <tr>\n      <td>7000</td>\n      <td>0.394000</td>\n    </tr>\n    <tr>\n      <td>7500</td>\n      <td>0.384900</td>\n    </tr>\n    <tr>\n      <td>8000</td>\n      <td>0.301400</td>\n    </tr>\n    <tr>\n      <td>8500</td>\n      <td>0.314800</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"\n\nTraining completed. Do not forget to share your model on huggingface.co/models =)\n\n\n","output_type":"stream"},{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=8729, training_loss=0.8286540756548977, metrics={'train_runtime': 6530.3131, 'train_samples_per_second': 42.77, 'train_steps_per_second': 1.337, 'total_flos': 1.980583004666683e+16, 'train_loss': 0.8286540756548977, 'epoch': 7.0})"},"metadata":{}}]},{"cell_type":"code","source":"trainer.save_model()","metadata":{"execution":{"iopub.status.busy":"2022-11-28T12:31:12.194648Z","iopub.execute_input":"2022-11-28T12:31:12.195042Z","iopub.status.idle":"2022-11-28T12:31:15.157410Z","shell.execute_reply.started":"2022-11-28T12:31:12.194989Z","shell.execute_reply":"2022-11-28T12:31:15.156457Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stderr","text":"Saving model checkpoint to ./results\nConfiguration saved in ./results/config.json\nModel weights saved in ./results/pytorch_model.bin\ntokenizer config file saved in ./results/tokenizer_config.json\nSpecial tokens file saved in ./results/special_tokens_map.json\n","output_type":"stream"}]},{"cell_type":"code","source":"import numpy as np","metadata":{"execution":{"iopub.status.busy":"2022-11-28T12:31:18.883281Z","iopub.execute_input":"2022-11-28T12:31:18.883650Z","iopub.status.idle":"2022-11-28T12:31:18.891546Z","shell.execute_reply.started":"2022-11-28T12:31:18.883616Z","shell.execute_reply":"2022-11-28T12:31:18.889563Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"def accuracy(preds, labels):\n    correct = 0\n    for i in range(len(preds)):\n        if(preds[i] == labels[i]):\n            correct += 1\n    return correct/len(preds)*100","metadata":{"execution":{"iopub.status.busy":"2022-11-28T12:31:23.156413Z","iopub.execute_input":"2022-11-28T12:31:23.157255Z","iopub.status.idle":"2022-11-28T12:31:23.164260Z","shell.execute_reply.started":"2022-11-28T12:31:23.157217Z","shell.execute_reply":"2022-11-28T12:31:23.163003Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"competition_x = (pd.read_csv(path + '/comp_test_x.csv'))['Title']\nfinal_dict_list = {}\nfinal_dict_list[\"label\"] = [0 for i in range(len(competition_x))]\nfinal_dict_list[\"text\"] = competition_x\ndata_final = Dataset.from_dict(final_dict_list)\ntokenized_final = data_final.map(preprocess_function, batched=True)\npredictions = trainer.predict(tokenized_final)","metadata":{"execution":{"iopub.status.busy":"2022-11-28T12:31:27.633637Z","iopub.execute_input":"2022-11-28T12:31:27.633995Z","iopub.status.idle":"2022-11-28T12:32:54.492220Z","shell.execute_reply.started":"2022-11-28T12:31:27.633962Z","shell.execute_reply":"2022-11-28T12:32:54.491202Z"},"trusted":true},"execution_count":17,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/12 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"258a01dd55a24f7fa168fd7308bb5fce"}},"metadata":{}},{"name":"stderr","text":"The following columns in the test set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Prediction *****\n  Num examples = 11514\n  Batch size = 32\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='360' max='360' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [360/360 01:25]\n    </div>\n    "},"metadata":{}}]},{"cell_type":"code","source":"preds = np.argmax(predictions.predictions, axis=-1)","metadata":{"execution":{"iopub.status.busy":"2022-11-28T12:33:02.215197Z","iopub.execute_input":"2022-11-28T12:33:02.215564Z","iopub.status.idle":"2022-11-28T12:33:02.233074Z","shell.execute_reply.started":"2022-11-28T12:33:02.215531Z","shell.execute_reply":"2022-11-28T12:33:02.229816Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stdout","text":"[ 8 20 27 ... 23 12  4]\n","output_type":"stream"}]},{"cell_type":"code","source":"id_value = []\ngenre = []\n\nfor i in range(len(preds)):\n    id_value.append(i)\n    genre.append(preds[i])\n\ndictCSV = {\"Id\" : id_value, 'Genre':genre}\ndf1 = pd.DataFrame(dictCSV)\ndf1.to_csv('comp_test_y.csv',index = False)","metadata":{"execution":{"iopub.status.busy":"2022-11-28T12:33:04.977820Z","iopub.execute_input":"2022-11-28T12:33:04.978219Z","iopub.status.idle":"2022-11-28T12:33:05.035698Z","shell.execute_reply.started":"2022-11-28T12:33:04.978182Z","shell.execute_reply":"2022-11-28T12:33:05.034753Z"},"trusted":true},"execution_count":19,"outputs":[]}]}